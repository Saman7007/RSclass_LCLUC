{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from seg_dataset import SegmentationDataset, RandomFlipRotate\n",
    "import segmentation_models_pytorch as smp \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from segformer_pytorch import Segformer\n",
    "import torch.nn.functional as F\n",
    "import os \n",
    "import glob\n",
    "from torchmetrics.classification.jaccard import MulticlassJaccardIndex as jaccard\n",
    "\n",
    "from matplotlib.colors import ListedColormap, LinearSegmentedColormap\n",
    "from captum.attr import LayerGradCam, LayerAttribution, visualization as viz\n",
    "from captum.attr import IntegratedGradients, Occlusion\n",
    "\n",
    "import os\n",
    "from skimage import exposure\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid decimal literal (2876559065.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[3], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    wget https://huggingface.co/ibm-nasa-geospatial/Prithvi-100M/resolve/main/Prithvi_100M.pt?download=true -O Prithvi_100M.pt\u001b[0m\n\u001b[0m                                                              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid decimal literal\n"
     ]
    }
   ],
   "source": [
    "wget https://huggingface.co/ibm-nasa-geospatial/Prithvi-100M/resolve/main/Prithvi_100M.pt?download=true -O Prithvi_100M.pt\n",
    "wget https://huggingface.co/ibm-nasa-geospatial/Prithvi-100M/resolve/main/Prithvi_100M_config.yaml?download=true -O Prithvi_100M_config.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: wget\n"
     ]
    }
   ],
   "source": [
    "! wget https://huggingface.co/ibm-nasa-geospatial/Prithvi-100M/resolve/main/Prithvi_100M.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_path = '/Users/sebrah13/Desktop/python_codes/prithvi-pytorch/tests/Prithvi_100M_config.yaml'\n",
    "ckpt_path = \"/Users/sebrah13/Desktop/python_codes/RSclass_LCLUC/weights/Prithvi_100M.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prithvi_pytorch import PrithviViT\n",
    "model = PrithviViT(\n",
    "    ckpt_path=ckpt_path,  # path to pretrained checkpoint Prithvi_100M.pt\n",
    "    cfg_path=cfg_path,  # path to pretrained config Prithvi_100M_config.yaml\n",
    "    num_classes=12,  # num classifier classes\n",
    "    in_chans=8,  # right now only supports the pretrained 6 channels\n",
    "    img_size=64,  # supports other image sizes than 224\n",
    "    freeze_encoder=True  # freeze the pretrained prithvi if you just want to linear probe\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 12])\n"
     ]
    }
   ],
   "source": [
    "X = torch.randn(1, 8, 64, 64)\n",
    "out = model(X)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prithvi_pytorch import PrithviEncoderDecoder\n",
    "\n",
    "model = PrithviEncoderDecoder(\n",
    "    ckpt_path=ckpt_path,  # path to pretrained checkpoint Prithvi_100M.pt\n",
    "    cfg_path=cfg_path,  # path to pretrained config Prithvi_100M_config.yaml\n",
    "    num_classes=9,  # num classifier classes\n",
    "    in_chans=8,  # right now only supports the pretrained 6 channels\n",
    "    img_size=64,  # supports other image sizes than 224\n",
    "    freeze_encoder=True  # freeze the pretrained prithvi\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "X = torch.randn(1, 8, 64, 64)\n",
    "out = model(X)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# %%\n",
    "EPOCHS = 2\n",
    "BS = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #%% Instantiate Dataset and Dataloader\n",
    "train_ds = SegmentationDataset(data_path='/scratch/sebrah13/RS_class/yearlyImage/Train')\n",
    "# sampler = torch.utils.data.WeightedRandomSampler(train_ds.weights, len(train_ds.weights))\n",
    "train_dataloader = DataLoader(train_ds, batch_size=BS, pin_memory=True)\n",
    "val_ds = SegmentationDataset(data_path='/scratch/sebrah13/RS_class/yearlyImage/Val')\n",
    "# sampler1 = torch.utils.data.WeightedRandomSampler(val_ds.weights, len(val_ds.weights))\n",
    "val_dataloader = DataLoader(val_ds, batch_size=BS, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for DD in val_dataloader:\n",
    "    print(DD['image'].shape, DD['mask'].shape)\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(DEVICE)\n",
    "optimizer = torch.optim.Adam([ \n",
    "    dict(params=model.parameters(), lr=0.001),\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "wights = glob.glob(\"SegFormer*.pth\")\n",
    "if wights:\n",
    "    model.load_state_dict(torch.load(f'SegFormer_epochs_{EPOCHS}_crossentropy_state_dict.pth'))\n",
    "    print(\"Pretrained weights loaded\")\n",
    "else:\n",
    "    print(\"No pretrained weights found, intializing random weights...\")    \n",
    "    # %%\n",
    "criterion = nn.CrossEntropyLoss().to(DEVICE)\n",
    "\n",
    "\n",
    "\n",
    "IoU = jaccard( num_classes= 9, average='none').to(DEVICE)\n",
    "\n",
    "train_losses, val_losses = [],[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Prev_loss = 10000\n",
    "min_loss = 10000\n",
    "for e in range(EPOCHS):\n",
    "    model.train()\n",
    "    running_train_loss, running_val_loss = 0, 0\n",
    "    metrics = {'iou_scores': [], 'f1_scores': [], 'f2_scores': [], 'accuracies': [], 'recalls': [], 'ious': [], 'losses': []}\n",
    "    for i, data in enumerate(train_dataloader):\n",
    "        #training phase\n",
    "        image_i, mask_i = data['image'], data['mask']\n",
    "        image = image_i.to(DEVICE)\n",
    "        mask = mask_i.to(DEVICE)\n",
    "        \n",
    "        # reset gradients\n",
    "        optimizer.zero_grad() \n",
    "        #forward\n",
    "        output = model(image.float())\n",
    "        # Upsample the output to match the target label size\n",
    "        # output_upsampled = output\n",
    "        output_upsampled =F.interpolate(output, size=mask.shape[1:], mode = 'bilinear',   align_corners=False)\n",
    "        # calc losses\n",
    "        train_loss = criterion(output_upsampled .float(), mask.long())\n",
    "\n",
    "        # back propagation\n",
    "        train_loss.backward()\n",
    "        optimizer.step() #update weight          \n",
    "        running_train_loss += train_loss.item()\n",
    "        \n",
    "        # adding metrics\n",
    "        IoU_metric = IoU(output_upsampled .float(), mask.long())\n",
    "        _, pred = torch.max(output_upsampled, 1)\n",
    "        tp, fp, fn, tn = smp.metrics.get_stats(pred, mask.long(), mode='multiclass', num_classes=9)\n",
    "        iou_score = smp.metrics.iou_score(tp, fp, fn, tn, reduction=\"micro\")\n",
    "        f1_score = smp.metrics.f1_score(tp, fp, fn, tn, reduction=\"micro\")\n",
    "        f2_score = smp.metrics.fbeta_score(tp, fp, fn, tn, beta=2, reduction=\"micro\")\n",
    "        accuracy = smp.metrics.accuracy(tp, fp, fn, tn, reduction=\"macro\")\n",
    "        recall = smp.metrics.recall(tp, fp, fn, tn, reduction=\"micro-imagewise\")\n",
    "        #storing the metrics in a dictionary\n",
    "        metrics['ious'].append(IoU_metric)\n",
    "        metrics['iou_scores'].append(iou_score)\n",
    "        metrics['f1_scores'].append(f1_score)\n",
    "        metrics['f2_scores'].append(f2_score)\n",
    "        metrics['accuracies'].append(accuracy)\n",
    "        metrics['recalls'].append(recall)\n",
    "        metrics['losses'].append(train_loss.item())\n",
    "        \n",
    "    train_losses.append(running_train_loss) \n",
    "    \n",
    "    # Compute mean of each metric\n",
    "    mean_metrics =  {\n",
    "    metric: np.mean([v.cpu().numpy() if isinstance(v, torch.Tensor) else v for v in values])\n",
    "    for metric, values in metrics.items()\n",
    "}\n",
    "    print(f\"Epoch: {e}, Training Mean Loss: {mean_metrics['losses']}, Mean IoU: {mean_metrics['ious']}, \"\n",
    "        f\"Mean IoU Score: {mean_metrics['iou_scores']}, Mean F1 Score: {mean_metrics['f1_scores']}, \")\n",
    "        # f\"Mean F2 Score: {mean_metrics['f2_scores']}, Mean Accuracy: {mean_metrics['accuracies']}, \"\n",
    "        # f\"Mean Recall: {mean_metrics['recalls']}\")\n",
    "    \n",
    "    # validation\n",
    "    model.eval()\n",
    "    val_metrics = {'iou_scores': [], 'f1_scores': [], 'f2_scores': [], 'accuracies': [], 'recalls': [], 'ious': []}\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(val_dataloader):\n",
    "            image_i, mask_i = data['image'], data['mask']\n",
    "            image = image_i.to(DEVICE)\n",
    "            mask = mask_i.to(DEVICE)\n",
    "            #forward\n",
    "            output = model(image.float())\n",
    "            # output_upsampled = output\n",
    "            \n",
    "            output_upsampled = F.interpolate(output, size=mask.shape[1:],mode = 'bilinear', align_corners=False)\n",
    "            # calc losses\n",
    "            val_loss = criterion(output_upsampled.float(), mask.long())\n",
    "            running_val_loss += val_loss.item()\n",
    "            \n",
    "            # Calculate additional metrics\n",
    "            _, pred = torch.max(output_upsampled, 1)\n",
    "            IoU_metric = IoU(output_upsampled.float(), mask.long())\n",
    "            tp, fp, fn, tn = smp.metrics.get_stats(pred, mask.long(), mode='multiclass', num_classes=9)\n",
    "            iou_score = smp.metrics.iou_score(tp, fp, fn, tn, reduction=\"micro\")\n",
    "            f1_score = smp.metrics.f1_score(tp, fp, fn, tn, reduction=\"micro\")\n",
    "            f2_score = smp.metrics.fbeta_score(tp, fp, fn, tn, beta=2, reduction=\"micro\")\n",
    "            accuracy = smp.metrics.accuracy(tp, fp, fn, tn, reduction=\"macro\")\n",
    "            recall = smp.metrics.recall(tp, fp, fn, tn, reduction=\"micro-imagewise\")\n",
    "            # Store metrics in the validation metrics dictionary\n",
    "            val_metrics['ious'].append(IoU_metric)\n",
    "            val_metrics['iou_scores'].append(iou_score)\n",
    "            val_metrics['f1_scores'].append(f1_score)\n",
    "            val_metrics['f2_scores'].append(f2_score)\n",
    "            val_metrics['accuracies'].append(accuracy)\n",
    "            val_metrics['recalls'].append(recall)\n",
    "            \n",
    "    val_losses.append(running_val_loss)\n",
    "    # Compute mean of each metric and loss\n",
    "    mean_val_metrics = {\n",
    "    metric: np.mean([v.cpu().numpy() if isinstance(v, torch.Tensor) else v for v in values])\n",
    "    for metric, values in val_metrics.items()\n",
    "}\n",
    "    mean_val_loss = np.mean(running_val_loss / len(val_dataloader))\n",
    "    # Append the average validation loss for this epoch\n",
    "    # val_losses.append(mean_val_loss)\n",
    "\n",
    "# Log or print validation metrics and loss\n",
    "    print(f\"Validation Loss: {mean_val_loss}, Mean IoU: {mean_val_metrics['ious']}, \"\n",
    "      f\"Mean IoU Score: {mean_val_metrics['iou_scores']}, Mean F1 Score: {mean_val_metrics['f1_scores']}, \")\n",
    "    #   f\"Mean F2 Score: {mean_val_metrics['f2_scores']}, Mean Accuracy: {mean_val_metrics['accuracies']}, \"\n",
    "    #   f\"Mean Recall: {mean_val_metrics['recalls']}\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    if np.median(running_val_loss) < min_loss:\n",
    "        print(f\"Loss value improved from {min_loss} to {np.median(running_val_loss)}; Saving model weights...\")\n",
    "        torch.save(model.state_dict(), f'Prithvi_epochs_{EPOCHS}_crossentropy_state_dict.pth')\n",
    "        Prev_loss = np.median(running_val_loss)\n",
    "        if min_loss > Prev_loss:\n",
    "            min_loss = Prev_loss\n",
    "            \n",
    "                # Write report to text file\n",
    "        with open('report.txt', 'a') as file:  # 'a' mode for appending in case this happens multiple times\n",
    "            file.write(f\"Epoch: {e}, Median Validation Loss: {running_train_loss},\\n\")\n",
    "            file.write(f\"Epoch: {e}, Median Validation Loss: {running_val_loss},\\n\")\n",
    "            file.write(\"Mean Validation Metrics:\\n\")\n",
    "            for metric, value in mean_val_metrics.items():\n",
    "                file.write(f\"{metric}: {value}\\n\")\n",
    "            file.write(\"Metrics training Criteria (if any):\\n\")\n",
    "            for metric, value in mean_metrics.items():\n",
    "                file.write(f\"{metric}: {value}\\n\")\n",
    "           \n",
    "    print(f\"Epoch: {e}: Train Cumulative Loss: {np.median(running_train_loss)}, Val cumulative Loss: {np.median(running_val_loss)} \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% TRAIN LOSS\n",
    "plt.figure(figsize=(10, 5))  \n",
    "sns.lineplot(x = range(len(train_losses)), y= train_losses)\n",
    "sns.lineplot(x = range(len(train_losses)), y= val_losses)\n",
    "\n",
    "# Adding titles and labels\n",
    "plt.title('Training vs Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()  # Show legend to identify the lines\n",
    "plt.savefig('trainloss.png')\n",
    "plt.show()  # Display the plotplt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dir = '/scratch/sebrah13/RS_class/yearlyImage/Val'\n",
    "test_ds = SegmentationDataset(data_path=test_dir)\n",
    "test_dataloader = DataLoader(test_ds, batch_size=1, shuffle=True)\n",
    "\n",
    "model_path = '/scratch/sebrah13/RS_class/LinkNET/LinkNet_epochs_200_crossentropy_state_dict.pth'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "model.to(DEVICE)\n",
    "model.load_state_dict(torch.load(model_path, map_location=torch.device(DEVICE)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Helper functions to calculate metrics\n",
    "def calculate_metrics(tp, fp, fn, tn):\n",
    "    epsilon = 1e-7\n",
    "    f1 = 2 * tp / (2 * tp + fp + fn + epsilon)\n",
    "    f2 = 5 * tp / (5 * tp + 4 * fn + fp + epsilon)\n",
    "    accuracy = (tp + tn) / (tp + tn + fp + fn + epsilon)\n",
    "    recall = tp / (tp + fn + epsilon)\n",
    "    return f1, f2, accuracy, recall\n",
    "\n",
    "# %% Model Evaluation\n",
    "num_classes = 9\n",
    "tp = np.zeros(num_classes)\n",
    "fp = np.zeros(num_classes)\n",
    "fn = np.zeros(num_classes)\n",
    "tn = np.zeros(num_classes)  # Added true negatives\n",
    "\n",
    "all_true_labels, all_pred_labels = [],[]\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in test_dataloader:\n",
    "        inputs, outputs = data['image'], data['mask']\n",
    "        true = outputs.to(torch.float32).to(DEVICE)\n",
    "        pred = model(inputs.to(DEVICE).float())\n",
    "        pred = F.interpolate(pred, size=true.shape[1:], mode = 'bilinear',   align_corners=False)\n",
    "        _, predicted = torch.max(pred, 1)\n",
    "        \n",
    "        all_true_labels.extend(true.cpu().numpy().flatten())\n",
    "        all_pred_labels.extend(predicted.cpu().numpy().flatten())\n",
    "\n",
    "        for cls in range(num_classes):\n",
    "            tp[cls] += torch.sum((predicted == cls) & (true == cls)).item()\n",
    "            fp[cls] += torch.sum((predicted == cls) & (true != cls)).item()\n",
    "            fn[cls] += torch.sum((predicted != cls) & (true == cls)).item()\n",
    "            tn[cls] += torch.sum((predicted != cls) & (true != cls)).item()  # Correctly count true negatives\n",
    "\n",
    "# Compute IoU for each class\n",
    "class_iou = tp / (tp + fp + fn + 1e-7)\n",
    "mean_iou = np.mean(class_iou)\n",
    "\n",
    "# Compute additional metrics\n",
    "f1_scores, f2_scores, accuracies, recalls = calculate_metrics(tp, fp, fn, tn)\n",
    "mean_f1 = np.mean(f1_scores)\n",
    "mean_f2 = np.mean(f2_scores)\n",
    "mean_accuracy = np.mean(accuracies)\n",
    "mean_recall = np.mean(recalls)\n",
    "\n",
    "print(f\"Class-wise IoUs: {class_iou}\")\n",
    "print(f\"Mean IoU: {mean_iou}\")\n",
    "print(f\"Mean F1 Score: {mean_f1}\")\n",
    "print(f\"Mean F2 Score: {mean_f2}\")\n",
    "print(f\"Mean Accuracy: {mean_accuracy}\")\n",
    "print(f\"Mean Recall: {mean_recall}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Confusion Matrix\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "Name = 'Linknet'\n",
    "\n",
    "# Compute confusion matrix\n",
    "conf_matrix = confusion_matrix(all_true_labels, all_pred_labels, labels=list(range(num_classes)))\n",
    "\n",
    "# Normalize confusion matrix by row (true labels)\n",
    "conf_matrix_normalized = conf_matrix.astype('float') / conf_matrix.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "# Convert to percentage\n",
    "conf_matrix_normalized *= 100\n",
    "# Create a custom colormap\n",
    "cmap = sns.color_palette(\"rocket_r\", as_cmap=True)\n",
    "# Plot normalized confusion matrix\n",
    "plt.figure(figsize=(6, 4.5))\n",
    "ax = sns.heatmap(pd.DataFrame(conf_matrix_normalized, columns=[f'{i+1}' for i in range(num_classes)], \n",
    "                         index=[f'{i+1}' for i in range(num_classes)]), \n",
    "            annot=True, fmt='.2f', cmap=cmap, vmin=0, vmax=100)\n",
    "\n",
    "# Set font properties\n",
    "plt.xlabel('Predicted', fontsize=14, fontname='Times New Roman')\n",
    "plt.ylabel('True', fontsize=14, fontname='Times New Roman')\n",
    "plt.title(f'{Name}'\n",
    "        #   {config_name[-5:]}'\n",
    "          , fontsize=18, fontname='Times New Roman')\n",
    "\n",
    "# Set ticks font properties\n",
    "ax.set_xticklabels(ax.get_xticklabels(), fontsize=14, fontname='Times New Roman')\n",
    "ax.set_yticklabels(ax.get_yticklabels(), fontsize=14, fontname='Times New Roman')\n",
    "\n",
    "# Adjust color bar font properties\n",
    "colorbar = ax.collections[0].colorbar\n",
    "colorbar.ax.tick_params(labelsize=14)\n",
    "colorbar.ax.set_yticklabels([f'{int(i)}%' for i in colorbar.get_ticks()], fontsize=14, fontname='Times New Roman')\n",
    "\n",
    "plt.savefig(f'normalized_confusion_matrix{Name}.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Pick a test image and show it\n",
    "from matplotlib.colors import ListedColormap\n",
    "Sample = next(iter(test_dataloader))\n",
    "image_test, mask = Sample['image'], Sample['mask']\n",
    "plt.imshow(np.transpose(image_test[0, 0:3, :, :].cpu().numpy(), (1, 2, 0)))\n",
    "\n",
    "#%% EVALUATE MODEL\n",
    "# create preds\n",
    "with torch.no_grad():\n",
    "    image_test = image_test.float().to(DEVICE)\n",
    "    output = model(image_test)\n",
    "\n",
    "#%%\n",
    "output_cpu = output.cpu().squeeze().numpy()\n",
    "Output = output_cpu[:,:,:]\n",
    "output_cpu = Output.transpose((1, 2, 0))\n",
    "output_cpu = output_cpu.argmax(axis=2)\n",
    "\n",
    "# %%\n",
    "\n",
    "\n",
    "\n",
    "# Define a color map with 9 distinct colors for values 0 to 8\n",
    "colors = ['black', 'red', 'blue', 'green', 'purple', 'orange', 'yellow', 'cyan', 'magenta']\n",
    "cmap = ListedColormap(colors)\n",
    "\n",
    "fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(10, 5))\n",
    "fig.suptitle('True and Predicted Mask')\n",
    "\n",
    "true_mask_img = axs[0].imshow(mask[0, :, :], cmap=cmap, vmin=0, vmax=8)\n",
    "predicted_mask_img = axs[1].imshow(output_cpu, cmap=cmap, vmin=0, vmax=8)\n",
    "\n",
    "# Add titles\n",
    "axs[0].set_title(\"True Mask\")\n",
    "axs[1].set_title(\"Predicted Mask\")\n",
    "\n",
    "# Add color bar to interpret the values\n",
    "fig.colorbar(true_mask_img, ax=axs, orientation='horizontal', fraction=0.05, pad=0.1, label='Class Values')\n",
    "\n",
    "# Save and display the plot\n",
    "plt.savefig('Predicted_Mask.png')\n",
    "plt.show()\n",
    "\n",
    "# %%\n",
    "print(np.unique(output_cpu))\n",
    "print(np.unique(mask[0, :, :].numpy()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
